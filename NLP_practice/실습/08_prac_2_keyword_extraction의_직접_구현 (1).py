# -*- coding: utf-8 -*-
"""08 Prac 2. Keyword Extraction의 직접 구현

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rtEOh-KEyOrPz5bmJ4KUur6m3Hcrgwar

# 8장. 핵심 키워드 추출 (Keyword Extraction)

# 8-0 데이터 준비

## Mecab 설치 (필요시)
"""

!sudo apt-get install g++ openjdk-7-jdk # Install Java 1.7+
!sudo apt-get install python-dev; pip install konlpy     # Python 2.x
!sudo apt-get install python3-dev; pip3 install konlpy   # Python 3.x
!sudo apt-get install curl
!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)

"""# 8-1 TF-IDF 활용 핵심키워드 추출

## 실습 1. sklearn 활용
"""

import requests 
from bs4 import BeautifulSoup

def get_news_by_url(url):
  res = requests.get(url)
  bs = BeautifulSoup(res.content, 'html.parser')

  title = bs.select('h3#articleTitle')[0].text #제목
  content = bs.select('#articleBodyContents')[0].get_text().replace('\n', " ") #본문
  content = content.replace("// flash 오류를 우회하기 위한 함수 추가 function _flash_removeCallback() {}", "")
  return  content.strip()

docs = []
docs.append( get_news_by_url('https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=105&oid=018&aid=0004430108') )
docs.append( get_news_by_url('https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=001&aid=0011614790') )
docs.append( get_news_by_url('https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=102&oid=014&aid=0004424362') )
docs.append( get_news_by_url('https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=119&aid=0002402191') )
docs.append( get_news_by_url('https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=101&oid=030&aid=0002882728') )
len(docs)

"""### 1) 전처리"""

from konlpy.tag import Mecab
mecab = Mecab()

preprocessed_docs = []
for doc in docs :
  # 명사와 동사만으로 문서 전처리
  preprocessed_docs.append(' '.join([token[0] for token in mecab.pos(doc) if token[1][0] in ['N', 'V']]))
preprocessed_docs[0][:100]

"""### 2) TF-IDF 계산"""

from sklearn.feature_extraction.text import CountVectorizer

count_vectorizer = CountVectorizer(max_df=0.85, max_features=10000)
word_count_vector = count_vectorizer.fit_transform(preprocessed_docs)
list(count_vectorizer.vocabulary_.keys())[:10]

from sklearn.feature_extraction.text import TfidfTransformer

tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)
tfidf_transformer.fit(word_count_vector)

"""### 3) 핵심키워드 추출"""

def sort_keywords(keywords):
    return sorted(zip(keywords.col, keywords.data), key=lambda x: (x[1], x[0]), reverse=True)
 
def extract_keywords(feature_names, sorted_keywords, n=5):
    return [(feature_names[idx], score) for idx, score in sorted_keywords[:n]]

doc = preprocessed_docs[0] # 핵심키워드 추출할 문서 조회

feature_names = count_vectorizer.get_feature_names() # TF-IDF 단어 목록
tf_idf_vector = tfidf_transformer.transform(count_vectorizer.transform([doc])) # 문서의 tf-idf 추출
sorted_keywords = sort_keywords(tf_idf_vector.tocoo()) # TF-IDF를 기준으로 역순 정렬
 
# 사용자가 지정한 갯수만큼 키워드 추출
keywords = extract_keywords(feature_names, sorted_keywords, 5)
 
print("\n===== 원문 =====")
print(docs[0][:100])
print("\n=== 핵심키워드 ===")
for k in keywords:
    print(k)

tf_idf_vector.tocoo().data

"""---

## 실습 2. gensim 활용

### 1) 전처리
"""

from konlpy.tag import Mecab
mecab = Mecab()

preprocessed_docs = []
for doc in docs :
  # 명사와 동사만으로 문서 전처리
  preprocessed_docs.append(' '.join([token[0] for token in mecab.pos(doc) if token[1][0] in ['N', 'V']]))
preprocessed_docs[0][:100]

"""### 2) TF-IDF 계산"""

from gensim.models import TfidfModel
from gensim.corpora import Dictionary

document_ls = [doc.split() for doc in preprocessed_docs]
dct = Dictionary(document_ls) # 인덱스(key) - 단어(valuue) 인 딕셔너리 생성
corpus = [dct.doc2bow(doc) for doc in document_ls] # 각 문서에 포함된 단어를 인덱스로 변환하여 corpus 생성
tfidf = TfidfModel(corpus) # TF-IDF 산출

"""### 3) 핵심키워드 추출"""

def sort_keywords(tfidf):
    return sorted(tfidf, key=lambda x: (x[1], x[0]), reverse=True)

def extract_keywords(feature_names, sorted_keywords, n=5):
    return [(feature_names[idx], score) for idx, score in sorted_keywords[:n]]

doc = corpus[0]

sorted_keywords = sort_keywords(tfidf[doc]) # TF-IDF를 기준으로 역순 정렬

# 사용자가 지정한 갯수만큼 키워드 추출
keywords = extract_keywords(dct, sorted_keywords, 5)

print("\n=== 핵심키워드 ===")
for k in keywords:
    print(k)

tfidf[doc]

"""---

# 8-2 Textrank
https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf

<img src="https://3.bp.blogspot.com/-yp0Lr3ec5EY/XIs6znCcO_I/AAAAAAAAAPY/xtZxe_OYtH0xeuWsp4Qd4DQrunGMpVQmQCLcBGAs/s640/keyword-extraction-textrank.png" />

## 실습 1. 행렬 활용

### 1) 토큰화 (Tokenization)

분석 텍스트 정제

### 2) Unique한 토큰 목록 생성

그래프 생성을 위해서 Unique한 토큰 목록 생성
"""

token = ['딸기', '바나나', '사과', '딸기', '파인애플']
nodes = ['바나나', '사과', '파인애플', '딸기']
vocab = nodes

vocab2idx = {vocab[i] : i for i in range(len(vocab))} #vocab을 인덱스로 변환
idx2vocab = {i : vocab[i] for i in range(len(vocab))} #인덱스를 vocab으로 변환
vocab2idx

len(token)

"""### 3) 그래프 생성 (weighted edge 계산)

*   TextRank는 그래프 기반 모델
*   각 단어(토큰)은 그래프의 노드(vertex) 
*   weighted_edge 행렬은 노드간 가중치 정보를 담고 있음
*   weighted_edge[i][j] 는 i번째 단어와 j번째 단어의 가중치를 의미
*   weighted_edge[i][j] 가 0인 경우는 노드간 연결이 없음을 의미
*   모든 노드는 1로 초기화
"""

import numpy as np
import math
vocab_len = len(vocab)

# 토큰별로 그래프 edge를 Matrix 형태로 생성
weighted_edge = np.zeros((vocab_len, vocab_len))

# 각 토큰 노드별로 스코어 1로 초기화
score = np.ones((vocab_len))



# coocurrence를 판단하기 위한 window 사이즈 설정
window_size = 2
covered_coocurrences = []

# weighted_edge 구현
for window_start in range(len(token) - window_size + 1):
  window = token[window_start : window_start + window_size]
  for i in range(window_size):
    for j in range(i + 1, window_size ):
      if window[i] in vocab2idx.keys() and window[j] in vocab2idx.keys():
        index_i = window_start + i
        index_j = window_start + j

        if [index_i, index_j] not in covered_coocurrences:
          weighted_edge[vocab2idx[window[i]]][vocab2idx[window[j]]] = 1
          weighted_edge[vocab2idx[window[j]]][vocab2idx[window[i]]] = 1
          covered_coocurrences.append([index_i, index_j])


for i in range(len(vocab)):
  sumi = weighted_edge[i].sum()
  weighted_edge[i] = weighted_edge[i]/sumi if sumi > 0 else 0
  



weighted_edge

"""### 4) 각 노드의 score계산
각 노드와 연결된 weighted edge의 값을 합산
"""

MAX_ITERATIONS = 50
d=0.85
threshold = 0.0001 #convergence threshold
score = np.ones((vocab_len))


for iter in range(0,MAX_ITERATIONS):
    prev_score = np.copy(score)
    
    for i in range(len(vocab)):
      summation = 0
      for j in range(len(vocab)):
        if weighted_edge[j][i] != 0:
          summation += weighted_edge[j][i] * score[j]

        score[i] = (1 -d) + d * (summation)

      if np.sum(np.fabs(prev_score - score)) <= threshold:
        break
score

"""### 5) 핵심 단어 추출"""

sorted_index = None

sorted_index = (np.argsort(score)[::-1])
n = 4
print("\n=== 핵심키워드 ===")
for i in range(n):
    print(str(idx2vocab[sorted_index[i]])+" : " + str(score[sorted_index[i]]))

sorted_index[0]

"""---"""

import requests 
from bs4 import BeautifulSoup

def get_news_by_url(url):
  res = requests.get(url)
  bs = BeautifulSoup(res.content, 'html.parser')

  title = bs.select('h3#articleTitle')[0].text #제목
  content = bs.select('#articleBodyContents')[0].get_text().replace('\n', " ") #본문
  content = content.replace("// flash 오류를 우회하기 위한 함수 추가 function _flash_removeCallback() {}", "")
  return  content.strip()

doc = get_news_by_url('https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=105&oid=018&aid=0004430108')
doc[:50]

"""### 1) 토큰화 (Tokenization)

분석 텍스트 정제
"""

from konlpy.tag import Mecab
mecab = Mecab()

nodes = [token for token in mecab.pos(doc) if token[1] in ['NNG','NNP']] #NNG, NNP를 스코어 계산 대상(노드)로 제한
tokens = [token for token in mecab.pos(doc)] #탐색할 토큰 전체

"""### 2) 그래프 생성 (weighted edge 계산)"""



"""### 3) 각 노드의 score계산"""



"""### 4)핵심 단어 추출"""

