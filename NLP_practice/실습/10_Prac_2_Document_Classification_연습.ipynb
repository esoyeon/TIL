{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ecOHtGBs7lDO"
   },
   "source": [
    "#10장 문서 분류 (Document Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MSHRKud41Tr0"
   },
   "source": [
    "#11-1 나이브 베이즈 분류(Naive Bayes Classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 직접구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "32FEohTU9qob"
   },
   "source": [
    "\n",
    "\n",
    "### 실습\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = [['me free lottery', 1],\n",
    " ['free get free you', 1],\n",
    " ['you free scholarship', 0],\n",
    " ['free to contact me', 0],\n",
    " ['you won award', 0],\n",
    " ['you ticket lottery', 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c8irQ-xFsq39"
   },
   "source": [
    "### 토큰 빈도수 및 문서별 토큰수 계산 (확률 계산을 위한 준비)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6MLSe8V1sYsv"
   },
   "source": [
    "![대체 텍스트](https://wikimedia.org/api/rest_v1/media/math/render/svg/98f086c560aa2f66650060277dda4f90e54e30c0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "for doc in training_set:\n",
    "    sent = doc[0].split(\" \")\n",
    "    for token in sent:\n",
    "        if token not in tokens:\n",
    "            tokens.append(token)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['me',\n",
       " 'free',\n",
       " 'lottery',\n",
       " 'get',\n",
       " 'you',\n",
       " 'scholarship',\n",
       " 'to',\n",
       " 'contact',\n",
       " 'won',\n",
       " 'award',\n",
       " 'ticket']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r4KjiGtqsbm_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# 범주에 속하는 토큰수 세기 1(스팸), 0(정상))\n",
    "doccnt0 = 0\n",
    "doccnt1 = 0\n",
    "\n",
    "for doc in training_set:\n",
    "    if 1 in doc:\n",
    "        doccnt0 += len(doc[0].split(\" \"))\n",
    "    else:\n",
    "        doccnt1 += len(doc[0].split(\" \"))\n",
    "        \n",
    "print(doccnt0, doccnt1)\n",
    "\n",
    "# 토큰별로 문서내 빈도수 카운팅\n",
    "wordfreq = defaultdict(lambda : [0, 0])\n",
    "\n",
    "for token in tokens:\n",
    "    for doc in training_set:\n",
    "        if token in doc[0] and doc[1] == 1:\n",
    "            wordfreq[token][1] += doc[0].count(token)\n",
    "        elif token in doc[0] and doc[1] == 0:\n",
    "            wordfreq[token][0] += doc[0].count(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'me': [1, 1],\n",
       "             'free': [2, 3],\n",
       "             'lottery': [0, 2],\n",
       "             'get': [0, 1],\n",
       "             'you': [2, 2],\n",
       "             'scholarship': [1, 0],\n",
       "             'to': [1, 0],\n",
       "             'contact': [1, 0],\n",
       "             'won': [1, 0],\n",
       "             'award': [1, 0],\n",
       "             'ticket': [0, 1]})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DpySoJWVISKw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doccnt0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EOaDmUrzI0IF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doccnt1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uKs_HaKwskLP"
   },
   "source": [
    "### Training : 토큰별 조건부 확률 계산 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dgCsmiCNsjjF"
   },
   "outputs": [],
   "source": [
    "k = 0.5\n",
    "\n",
    "wordprobs = defaultdict(lambda : [0, 0])\n",
    "\n",
    "for token in tokens:\n",
    "    wordprobs[token][0] = (k + wordfreq[token][0]) / (2*k +doccnt0)\n",
    "    wordprobs[token][1] = (k + wordfreq[token][1]) / (2*k +doccnt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'me': [0.13636363636363635, 0.13636363636363635],\n",
       "             'free': [0.22727272727272727, 0.3181818181818182],\n",
       "             'lottery': [0.045454545454545456, 0.22727272727272727],\n",
       "             'get': [0.045454545454545456, 0.13636363636363635],\n",
       "             'you': [0.22727272727272727, 0.22727272727272727],\n",
       "             'scholarship': [0.13636363636363635, 0.045454545454545456],\n",
       "             'to': [0.13636363636363635, 0.045454545454545456],\n",
       "             'contact': [0.13636363636363635, 0.045454545454545456],\n",
       "             'won': [0.13636363636363635, 0.045454545454545456],\n",
       "             'award': [0.13636363636363635, 0.045454545454545456],\n",
       "             'ticket': [0.045454545454545456, 0.13636363636363635]})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordprobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Viq9UFbs3Y2"
   },
   "source": [
    "### Classify : 신규 텍스트가 주어졌을 때 확률 계산\n",
    "\n",
    ">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22727272727272727"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordprobs['lottery'][1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ypuyUtI4LAQs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.1451323043030026\n",
      "-1.4816045409242156\n",
      "-2.626736845227218\n",
      "-4.572646994282531\n",
      "정상확률 : 0.12500000000000008\n",
      "스팸확률 : 0.8749999999999999\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "doc = \"free lottery\"\n",
    "\n",
    "tokens = doc.split(\" \")\n",
    "spam_log_prob = math.log(doccnt1 / (doccnt0+doccnt1))\n",
    "normal_log_prob = math.log(doccnt0 / (doccnt0+doccnt1))\n",
    "\n",
    "spam_log_sum, normal_log_sum = 0, 0\n",
    "for token in tokens:\n",
    "    spam_log_sum += math.log(wordprobs[token][1])\n",
    "    print(spam_log_sum)\n",
    "    normal_log_sum += math.log(wordprobs[token][0])\n",
    "    print(normal_log_sum)\n",
    "spam_log_sum = spam_log_sum + spam_log_prob\n",
    "normal_log_sum = normal_log_sum + normal_log_prob\n",
    "\n",
    "exp_spam = math.exp(spam_log_sum)\n",
    "exp_normal = math.exp(normal_log_sum)\n",
    "\n",
    "prob_spam = exp_spam / (exp_spam+exp_normal)\n",
    "prob_normal = exp_normal / (exp_spam + exp_normal)\n",
    "    \n",
    "print(f\"정상확률 : {prob_normal}\")\n",
    "print(f\"스팸확률 : {prob_spam}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.6931471805599453, -0.6931471805599453)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_log_prob, normal_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.3198840257871636, -5.265794174842476)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_log_sum, normal_log_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# 쌤ver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Basian Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = [['me free lottery', 1],\n",
    " ['free get free you', 1],\n",
    " ['you free scholarship', 0],\n",
    " ['free to contact me', 0],\n",
    " ['you won award', 0],\n",
    " ['you ticket lottery', 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰 빈도수 및 문서별 토큰수 계산 (확률 계산을 위한 준비)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doccnt0: 10\n",
      "doccnt1: 10\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# 범주에 속하는 토큰 수 세기 1(스팸), 0(정상)\n",
    "\n",
    "doccnt0 = 0 \n",
    "doccnt1 = 0 \n",
    "\n",
    "#토큰 별로 문서내 빈도수 카운팅\n",
    "wordfreq = defaultdict(lambda:[0,0])\n",
    "\n",
    "for doc, label in training_set:\n",
    "    words = doc.split()\n",
    "    for word in words:\n",
    "        wordfreq[word][label] += 1 \n",
    "\n",
    "for key, (cnt0, cnt1) in wordfreq.items():\n",
    "    doccnt0 += cnt0\n",
    "    doccnt1 += cnt1\n",
    "    \n",
    "print('doccnt0: {}'.format(doccnt0))\n",
    "print('doccnt1: {}'.format(doccnt1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training : 토큰별 조건부 확률 계산 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'me': [0.13636363636363635, 0.13636363636363635],\n",
       "             'free': [0.22727272727272727, 0.3181818181818182],\n",
       "             'lottery': [0.045454545454545456, 0.22727272727272727],\n",
       "             'get': [0.045454545454545456, 0.13636363636363635],\n",
       "             'you': [0.22727272727272727, 0.22727272727272727],\n",
       "             'scholarship': [0.13636363636363635, 0.045454545454545456],\n",
       "             'to': [0.13636363636363635, 0.045454545454545456],\n",
       "             'contact': [0.13636363636363635, 0.045454545454545456],\n",
       "             'won': [0.13636363636363635, 0.045454545454545456],\n",
       "             'award': [0.13636363636363635, 0.045454545454545456],\n",
       "             'ticket': [0.045454545454545456, 0.13636363636363635]})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 0.5\n",
    "\n",
    "wordprobs = defaultdict(lambda: [0, 0])\n",
    "for key, (cnt0, cnt1) in wordfreq.items():\n",
    "    wordprobs[key][0] = (cnt0 + k)/(2*k +doccnt0)\n",
    "    wordprobs[key][1] = (cnt1 + k) / (2*k +doccnt1)\n",
    "    \n",
    "wordprobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify : 신규 텍스트가 주어졌을 때 확률 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정상확률 :87.49999999999999\n",
      "스팸확률 :12.500000000000009\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "doc = \"free lottery\"\n",
    "tokens = doc.split(' ')\n",
    "\n",
    "log_prob1 = log_prob0 = 0.0\n",
    "\n",
    "for word, (prob1, prob0) in wordprobs.items():\n",
    "    if word in tokens:\n",
    "        log_prob0 += math.log(prob0)\n",
    "        log_prob1 += math.log(prob1)\n",
    "        \n",
    "log_prob0 += math.log(doccnt0 / (doccnt0 + doccnt1))\n",
    "log_prob1 += math.log(doccnt1 / (doccnt0 + doccnt1))\n",
    "\n",
    "prob0 = math.exp(log_prob0)\n",
    "prob1 = math.exp(log_prob1)\n",
    "\n",
    "print(\"정상확률 :{}\".format(prob0 / (prob0+prob1) * 100))\n",
    "print(\"스팸확률 :{}\".format(prob1 / (prob0+prob1) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset = 'train', shuffle = True)\n",
    "\n",
    "print(twenty_train.target_names)\n",
    "print(twenty_train.data[0])\n",
    "print(twenty_train.target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "\n",
    "#변수 분류가 이진이 아니라 여러개니까 \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7738980350504514"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "twenty_test = fetch_20newsgroups(subset = 'test', shuffle = True)\n",
    "predicted = text_clf.predict(twenty_test.data)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정확도가 0.77이 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  4.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score : 0.9157684864695698\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_parameters_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-347d07dae187>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mgs_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgs_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtwenty_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtwenty_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Best score : {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgs_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Best parameters : {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgs_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_parameters_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_parameters_'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters_clf = {'vect__ngram_range':[(1,1), (1,2)],\n",
    "                  'tfidf__use_idf':(True, False),\n",
    "                  'clf__alpha':(1, 0.1, 0.01, 0.001, 0.0001)\n",
    "                 }\n",
    "\n",
    "# 위 파이프라인에 들어있는 parameter들을 변경해서 best를 search 하는 것\n",
    "\n",
    "gs_clf = GridSearchCV(text_clf, parameters_clf, n_jobs=-1, verbose = 2)\n",
    "gs_clf = gs_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score : 0.9157684864695698\n",
      "\tclf : MultinomialNB(alpha=0.001, class_prior=None, fit_prior=True)\n",
      "\tclf__alpha : 0.001\n",
      "\tclf__class_prior : None\n",
      "\tclf__fit_prior : True\n",
      "\tmemory : None\n",
      "\tsteps : [('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=0.001, class_prior=None, fit_prior=True))]\n",
      "\ttfidf : TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)\n",
      "\ttfidf__norm : l2\n",
      "\ttfidf__smooth_idf : True\n",
      "\ttfidf__sublinear_tf : False\n",
      "\ttfidf__use_idf : True\n",
      "\tvect : CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "                ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
      "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None)\n",
      "\tvect__analyzer : word\n",
      "\tvect__binary : False\n",
      "\tvect__decode_error : strict\n",
      "\tvect__dtype : <class 'numpy.int64'>\n",
      "\tvect__encoding : utf-8\n",
      "\tvect__input : content\n",
      "\tvect__lowercase : True\n",
      "\tvect__max_df : 1.0\n",
      "\tvect__max_features : None\n",
      "\tvect__min_df : 1\n",
      "\tvect__ngram_range : (1, 2)\n",
      "\tvect__preprocessor : None\n",
      "\tvect__stop_words : None\n",
      "\tvect__strip_accents : None\n",
      "\tvect__token_pattern : (?u)\\b\\w\\w+\\b\n",
      "\tvect__tokenizer : None\n",
      "\tvect__vocabulary : None\n",
      "\tverbose : False\n"
     ]
    }
   ],
   "source": [
    "print('Best score : {}'.format(gs_clf.best_score_))\n",
    "best_parameters = gs_clf.best_estimator_.get_params()\n",
    "for param_name in sorted(list(best_parameters.keys())):\n",
    "  print(\"\\t{} : {}\".format(param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8361656930430165"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = gs_clf.best_estimator_.predict(twenty_test.data)\n",
    "np.mean(predicted == twenty_test.target)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "10 Prac 2. Document Classification 연습",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
